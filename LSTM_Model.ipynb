{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vGklta7e2xur"},"source":["# Assumptions (and Notes)\n","\n","1. Temperature data was not in half-hourly intervals, so we applied the pandas .resample() method.\n","2. The .resample() method will add any missing half-hourly intervals not initially present in the data\n","3. If a day (or day that led into another day or days) that had:\n","  * less then 10 consecutive missing values, we applied the linear interpolator to impute those missing values\n","    * Days that had 9 or less consecutive missing values didn't lose much information when we applied the linear interpolator. The reason being is that the majority of days that did have missing values only really had 1-3 consecutive half-hourly intervals missing, and they weren't during the peak period, more so in the morning/evening.\n","  * greater than or equal to 10 consecutive missing values we just removed that day (or those days).\n","    * Days that had in excess of 10 consecutive missing values, we found hard to apply any interpolation without causing any bias. For example, if times from 10:30am to 6:30pm were missing, and we applied the linear interpolator, then we would miss valuable information during the middle of the day when the temperature usually reaches its peak. Thats why we just removed the entire day."]},{"cell_type":"markdown","metadata":{"id":"2R58Ijx3G-LL"},"source":["# Loading Libraries and Data"]},{"cell_type":"code","metadata":{"id":"vmTrl7iiKbYn","executionInfo":{"status":"ok","timestamp":1638187207319,"user_tz":-660,"elapsed":3444,"user":{"displayName":"Jake Keenan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02374688406366167760"}}},"source":["import pandas as pd\n","import numpy as np\n","import datetime as dt\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from tensorflow import keras\n","from numpy import array"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGpRoibLK2w6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638187225780,"user_tz":-660,"elapsed":18469,"user":{"displayName":"Jake Keenan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02374688406366167760"}},"outputId":"f218f817-f214-4d33-bbea-2a567b5235cc"},"source":["#######################################################\n","############### If using Google Drive #################\n","#######################################################\n","\n","# Mount Google Drive where datasets are located\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Please note that if this cell does not run go to where the shared folder is on Google Drive, and\n","# right-click on the shared folder, and select Add shortcut to Drive. Then try execute the cell again.\n","\n","# Change the current working directory\n","%cd /content/gdrive/MyDrive/DS\\ Capstone\\ Project/REPORT_CODE_STRUCTURED/data/\n","\n","# Define data path\n","data_path = \"processed/\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/.shortcut-targets-by-id/107U69c8Nh3fH_vc0lG1KAZi0i92sHvcb/DS Capstone Project/REPORT_CODE_STRUCTURED/data\n"]}]},{"cell_type":"code","metadata":{"id":"HzDLIoPsFyub"},"source":["#######################################################\n","############### If using Local Computer ###############\n","#######################################################\n","\n","# Define data path\n","data_path = \"../data/processed/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-E1KOASUYc1I","executionInfo":{"status":"ok","timestamp":1638187229124,"user_tz":-660,"elapsed":1642,"user":{"displayName":"Jake Keenan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02374688406366167760"}}},"source":["# Import the data\n","df = pd.read_csv(f'{data_path}data_used_to_build_model.csv')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6mUKbduKko6","executionInfo":{"status":"ok","timestamp":1638187232408,"user_tz":-660,"elapsed":418,"user":{"displayName":"Jake Keenan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02374688406366167760"}}},"source":["# Filter just for region of QLD\n","df_qld = df[df['REGIONID'] == 'QLD'].reset_index(drop=True).copy()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpftfx2FGn-T","executionInfo":{"status":"ok","timestamp":1638187232410,"user_tz":-660,"elapsed":8,"user":{"displayName":"Jake Keenan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02374688406366167760"}},"outputId":"6468fe28-0654-4270-f741-c4575c7c5e68"},"source":["# Check data info\n","df_qld.info()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 195697 entries, 0 to 195696\n","Data columns (total 4 columns):\n"," #   Column       Non-Null Count   Dtype  \n","---  ------       --------------   -----  \n"," 0   DATETIME     195697 non-null  object \n"," 1   TEMPERATURE  195697 non-null  float64\n"," 2   REGIONID     195697 non-null  object \n"," 3   TOTALDEMAND  195697 non-null  float64\n","dtypes: float64(2), object(2)\n","memory usage: 6.0+ MB\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Phx25CZGuAG","executionInfo":{"status":"ok","timestamp":1638171712041,"user_tz":-480,"elapsed":13,"user":{"displayName":"Honzik Jurza","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04286314741626610302"}},"outputId":"414e4cef-dda9-46d6-b303-7502dc7b347f"},"source":["# Print first and last 2 rows of DataFrame\n","head_tail = [df_qld.head(2), df_qld.tail(2)]\n","for i in head_tail:\n","  print(f'{i}\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              DATETIME  TEMPERATURE REGIONID  TOTALDEMAND\n","0  2010-01-01 00:00:00         23.6      QLD      5561.21\n","1  2010-01-01 00:30:00         23.7      QLD      5422.25\n","\n","                   DATETIME  TEMPERATURE REGIONID  TOTALDEMAND\n","195695  2021-03-17 23:30:00         19.6      QLD      5897.64\n","195696  2021-03-18 00:00:00         19.5      QLD      5737.03\n","\n"]}]},{"cell_type":"code","metadata":{"id":"1QhjmVNZVqbW"},"source":["# Change DATETIME to data type datetime64[ns]\n","df_qld.loc[:, 'DATETIME'] = pd.to_datetime(df_qld.loc[:, 'DATETIME'], format='%Y-%m-%d %H:%M:%S')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dKI7MaPgWQf2"},"source":["# Feature Selection/Engineering"]},{"cell_type":"code","metadata":{"id":"9mS-mEqHXB4M"},"source":["### Create 2 copies of the DataFrame\n","\n","# First copy used for machine learning\n","dfm = df_qld.copy()\n","\n","# Second copy used to add back predictions from model at end\n","dfm2 = df_qld.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2TuIJ2YWf_l"},"source":["# Create new features\n","def create_new_features(df):\n","    df[\"HOUR\"] = df[\"DATETIME\"].dt.hour\n","    df[\"ISO_DAYOFWEEK\"] = df[\"DATETIME\"].dt.isocalendar().day\n","    df[\"MONTH\"] = df[\"DATETIME\"].dt.month\n","    df[\"QUARTER\"] = df[\"DATETIME\"].dt.quarter\n","    df[\"DAYOFYEAR\"] = df[\"DATETIME\"].dt.dayofyear\n","    df[\"DAYOFMONTH\"] = df[\"DATETIME\"].dt.day\n","    df[\"ISO_WEEKOFYEAR\"] = df[\"DATETIME\"].dt.isocalendar().week\n","    df[\"SEASON\"] = df[\"DATETIME\"].dt.month%12 // 3 + 1 # Season 1 is Summer\n","    return df\n","\n","dfm = create_new_features(dfm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AHevv8yf-1V","outputId":"8e0f8c5f-a90e-4ad9-86b2-b77619ac7929"},"source":["# Let's check the new column Dtypes\n","dfm.info()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 195697 entries, 0 to 195696\n","Data columns (total 12 columns):\n"," #   Column          Non-Null Count   Dtype         \n","---  ------          --------------   -----         \n"," 0   DATETIME        195697 non-null  datetime64[ns]\n"," 1   TEMPERATURE     195697 non-null  float64       \n"," 2   REGIONID        195697 non-null  object        \n"," 3   TOTALDEMAND     195697 non-null  float64       \n"," 4   HOUR            195697 non-null  int64         \n"," 5   ISO_DAYOFWEEK   195697 non-null  UInt32        \n"," 6   MONTH           195697 non-null  int64         \n"," 7   QUARTER         195697 non-null  int64         \n"," 8   DAYOFYEAR       195697 non-null  int64         \n"," 9   DAYOFMONTH      195697 non-null  int64         \n"," 10  ISO_WEEKOFYEAR  195697 non-null  UInt32        \n"," 11  SEASON          195697 non-null  int64         \n","dtypes: UInt32(2), datetime64[ns](1), float64(2), int64(6), object(1)\n","memory usage: 16.8+ MB\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"ysSpgakgJqDS","outputId":"cd546d7c-65ee-4aca-8f2d-eb26bc0bbccc"},"source":["# Check to make sure that:\n","# (1) there are no values below 0, otherwise MinMaxScaler won't work\n","# (2) the data types are correct\n","# (3) we can change the data type to a lower bit number for performance \n","dfm.describe()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TEMPERATURE</th>\n","      <th>TOTALDEMAND</th>\n","      <th>HOUR</th>\n","      <th>ISO_DAYOFWEEK</th>\n","      <th>MONTH</th>\n","      <th>QUARTER</th>\n","      <th>DAYOFYEAR</th>\n","      <th>DAYOFMONTH</th>\n","      <th>ISO_WEEKOFYEAR</th>\n","      <th>SEASON</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","      <td>195697.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>20.413124</td>\n","      <td>6026.646686</td>\n","      <td>11.499941</td>\n","      <td>4.002453</td>\n","      <td>6.435106</td>\n","      <td>2.480493</td>\n","      <td>180.436558</td>\n","      <td>15.687282</td>\n","      <td>26.238333</td>\n","      <td>2.478536</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>5.635223</td>\n","      <td>868.846227</td>\n","      <td>6.922235</td>\n","      <td>1.999140</td>\n","      <td>3.480079</td>\n","      <td>1.126503</td>\n","      <td>106.388310</td>\n","      <td>8.786911</td>\n","      <td>15.210316</td>\n","      <td>1.120130</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.300000</td>\n","      <td>3748.240000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>16.600000</td>\n","      <td>5368.680000</td>\n","      <td>5.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>87.000000</td>\n","      <td>8.000000</td>\n","      <td>13.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>20.900000</td>\n","      <td>5993.390000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","      <td>6.000000</td>\n","      <td>2.000000</td>\n","      <td>179.000000</td>\n","      <td>16.000000</td>\n","      <td>26.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>24.400000</td>\n","      <td>6601.630000</td>\n","      <td>17.000000</td>\n","      <td>6.000000</td>\n","      <td>9.000000</td>\n","      <td>3.000000</td>\n","      <td>273.000000</td>\n","      <td>23.000000</td>\n","      <td>39.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>42.400000</td>\n","      <td>9988.090000</td>\n","      <td>23.000000</td>\n","      <td>7.000000</td>\n","      <td>12.000000</td>\n","      <td>4.000000</td>\n","      <td>366.000000</td>\n","      <td>31.000000</td>\n","      <td>53.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         TEMPERATURE    TOTALDEMAND  ...  ISO_WEEKOFYEAR         SEASON\n","count  195697.000000  195697.000000  ...   195697.000000  195697.000000\n","mean       20.413124    6026.646686  ...       26.238333       2.478536\n","std         5.635223     868.846227  ...       15.210316       1.120130\n","min         1.300000    3748.240000  ...        1.000000       1.000000\n","25%        16.600000    5368.680000  ...       13.000000       1.000000\n","50%        20.900000    5993.390000  ...       26.000000       2.000000\n","75%        24.400000    6601.630000  ...       39.000000       3.000000\n","max        42.400000    9988.090000  ...       53.000000       4.000000\n","\n","[8 rows x 10 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"yMGGPOuBXTtj"},"source":["# Remove unnecessary columns - only contains the value QLD\n","dfm.drop(columns=['REGIONID'], axis=1, inplace=True)\n","\n","# Transforming feature data types to make DataFrame smaller in size for performance\n","### int8 - handle values between -128 to 127\n","### int16 - handle values between -32768 to 32767 \n","dfm = dfm.astype({'HOUR': 'int8',\n","                 'ISO_DAYOFWEEK': 'int8',\n","                 'MONTH': 'int8',\n","                 'QUARTER': 'int8',\n","                 'DAYOFYEAR': 'int16',\n","                 'DAYOFMONTH': 'int8',\n","                 'ISO_WEEKOFYEAR': 'int8',\n","                 'SEASON': 'int8'})\n","\n","# Changing positioning of columns (needed as split_sequences function depends on positioning)\n","# These four lines of code will make DATETIME and TOTALDEMAND the first and second feature respectively\n","col = dfm.pop(\"TOTALDEMAND\")\n","dff = dfm.insert(0, col.name, col)\n","col = dfm.pop(\"DATETIME\")\n","dff = dfm.insert(0, col.name, col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"kaoyN5vPXj5x","outputId":"60eeb45a-01d3-42db-f334-1f9eecdb945b"},"source":["# Checking the data\n","dfm.head(2)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>TEMPERATURE</th>\n","      <th>HOUR</th>\n","      <th>ISO_DAYOFWEEK</th>\n","      <th>MONTH</th>\n","      <th>QUARTER</th>\n","      <th>DAYOFYEAR</th>\n","      <th>DAYOFMONTH</th>\n","      <th>ISO_WEEKOFYEAR</th>\n","      <th>SEASON</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2010-01-01 00:00:00</td>\n","      <td>5561.21</td>\n","      <td>23.6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2010-01-01 00:30:00</td>\n","      <td>5422.25</td>\n","      <td>23.7</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             DATETIME  TOTALDEMAND  ...  ISO_WEEKOFYEAR  SEASON\n","0 2010-01-01 00:00:00      5561.21  ...              53       1\n","1 2010-01-01 00:30:00      5422.25  ...              53       1\n","\n","[2 rows x 11 columns]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"xLEBJRELZPfb"},"source":["# Creates 6 new features, a 30, 60, 90, 120, 150, and 180 lagged version of TOTALDEMAND feature - these 6 will be our response\n","for i in range(1, 7):\n","  dfm[f'plus_{i*30}'] = dfm['TOTALDEMAND'].shift(-1*i)\n","\n","# Drop the rows with missing values\n","dfm.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"C3EBxWNi0hI7","outputId":"b915ba49-de8b-4650-a3cb-3fb3011b5b25"},"source":["# Observe the new data\n","dfm"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>TEMPERATURE</th>\n","      <th>HOUR</th>\n","      <th>ISO_DAYOFWEEK</th>\n","      <th>MONTH</th>\n","      <th>QUARTER</th>\n","      <th>DAYOFYEAR</th>\n","      <th>DAYOFMONTH</th>\n","      <th>ISO_WEEKOFYEAR</th>\n","      <th>SEASON</th>\n","      <th>plus_30</th>\n","      <th>plus_60</th>\n","      <th>plus_90</th>\n","      <th>plus_120</th>\n","      <th>plus_150</th>\n","      <th>plus_180</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2010-01-01 00:00:00</td>\n","      <td>5561.21</td>\n","      <td>23.6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5422.25</td>\n","      <td>5315.98</td>\n","      <td>5186.70</td>\n","      <td>5050.83</td>\n","      <td>4924.74</td>\n","      <td>4833.84</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2010-01-01 00:30:00</td>\n","      <td>5422.25</td>\n","      <td>23.7</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5315.98</td>\n","      <td>5186.70</td>\n","      <td>5050.83</td>\n","      <td>4924.74</td>\n","      <td>4833.84</td>\n","      <td>4815.04</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2010-01-01 01:00:00</td>\n","      <td>5315.98</td>\n","      <td>23.5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5186.70</td>\n","      <td>5050.83</td>\n","      <td>4924.74</td>\n","      <td>4833.84</td>\n","      <td>4815.04</td>\n","      <td>4816.91</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2010-01-01 01:30:00</td>\n","      <td>5186.70</td>\n","      <td>22.6</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5050.83</td>\n","      <td>4924.74</td>\n","      <td>4833.84</td>\n","      <td>4815.04</td>\n","      <td>4816.91</td>\n","      <td>4791.08</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2010-01-01 02:00:00</td>\n","      <td>5050.83</td>\n","      <td>22.0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>4924.74</td>\n","      <td>4833.84</td>\n","      <td>4815.04</td>\n","      <td>4816.91</td>\n","      <td>4791.08</td>\n","      <td>4772.35</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195686</th>\n","      <td>2021-03-17 19:00:00</td>\n","      <td>7486.67</td>\n","      <td>20.1</td>\n","      <td>19</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7327.30</td>\n","      <td>7190.86</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","    </tr>\n","    <tr>\n","      <th>195687</th>\n","      <td>2021-03-17 19:30:00</td>\n","      <td>7327.30</td>\n","      <td>19.8</td>\n","      <td>19</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7190.86</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","    </tr>\n","    <tr>\n","      <th>195688</th>\n","      <td>2021-03-17 20:00:00</td>\n","      <td>7190.86</td>\n","      <td>19.8</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","    </tr>\n","    <tr>\n","      <th>195689</th>\n","      <td>2021-03-17 20:30:00</td>\n","      <td>7086.78</td>\n","      <td>19.8</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","      <td>5897.64</td>\n","    </tr>\n","    <tr>\n","      <th>195690</th>\n","      <td>2021-03-17 21:00:00</td>\n","      <td>6894.85</td>\n","      <td>19.8</td>\n","      <td>21</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","      <td>5897.64</td>\n","      <td>5737.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>195691 rows × 17 columns</p>\n","</div>"],"text/plain":["                  DATETIME  TOTALDEMAND  ...  plus_150  plus_180\n","0      2010-01-01 00:00:00      5561.21  ...   4924.74   4833.84\n","1      2010-01-01 00:30:00      5422.25  ...   4833.84   4815.04\n","2      2010-01-01 01:00:00      5315.98  ...   4815.04   4816.91\n","3      2010-01-01 01:30:00      5186.70  ...   4816.91   4791.08\n","4      2010-01-01 02:00:00      5050.83  ...   4791.08   4772.35\n","...                    ...          ...  ...       ...       ...\n","195686 2021-03-17 19:00:00      7486.67  ...   6651.18   6443.62\n","195687 2021-03-17 19:30:00      7327.30  ...   6443.62   6264.63\n","195688 2021-03-17 20:00:00      7190.86  ...   6264.63   6144.16\n","195689 2021-03-17 20:30:00      7086.78  ...   6144.16   5897.64\n","195690 2021-03-17 21:00:00      6894.85  ...   5897.64   5737.03\n","\n","[195691 rows x 17 columns]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"Q3xrRDnpH1KP"},"source":["# Define model start DATETIME - we only use data from 2016 onwards to build our model\n","model_start_time = '2016-01-01 00:00:00'\n","\n","# Define the train/test cutoff DATETIME - used just for final model once we find optimal hyperparameters \n","trainTestSplit = '2020-01-01 00:00:00'\n","\n","# Discard all rows with DATETIME before 'model_start_time' - earlier dates are not an accurate representation of later dates\n","dfm = dfm[dfm['DATETIME'] >= model_start_time].reset_index(drop=True)\n","\n","# We also make a further 3 DataFrames for 3-Fold Nested CV\n","### First we define the endtimes for train/val/test splits\n","train_end_1618, val_end_1618, test_end_1618 = '2016-12-31 23:30:00', '2017-12-31 23:30:00', '2018-12-31 23:30:00'\n","train_end_1619, val_end_1619, test_end_1619 = '2017-12-31 23:30:00', '2018-12-31 23:30:00', '2019-12-31 23:30:00'\n","train_end_1620, val_end_1620, test_end_1620 = '2018-12-31 23:30:00', '2019-12-31 23:30:00', '2021-03-18 00:00:00'\n","\n","### Define the 3 CV DataFrames\n","df_1618 = dfm.loc[(model_start_time <= dfm['DATETIME']) & (dfm['DATETIME'] <= test_end_1618), :].copy()\n","df_1619 = dfm.loc[(model_start_time <= dfm['DATETIME']) & (dfm['DATETIME'] <= test_end_1619), :].copy()\n","df_1620 = dfm.loc[(model_start_time <= dfm['DATETIME']) & (dfm['DATETIME'] <= test_end_1620), :].copy()\n","\n","# Create a copy\n","dfm3 = dfm.copy()\n","dfm3.index = dfm3['DATETIME']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmDHyvN3Xox7"},"source":["# Columns to scale (where adding the six lagged TOTALDEMAND features)\n","cols = ['TOTALDEMAND', 'TEMPERATURE', 'HOUR', 'ISO_DAYOFWEEK', 'MONTH', 'QUARTER', 'DAYOFYEAR', 'DAYOFMONTH', 'ISO_WEEKOFYEAR', 'SEASON'] + list(dfm.columns[11:])\n","\n","# Scale main DataFrame - used for final model once we find optimal hyperparameters\n","scaler = MinMaxScaler()\n","dfm.loc[dfm['DATETIME'] < trainTestSplit, cols] = scaler.fit_transform(dfm.loc[dfm['DATETIME'] < trainTestSplit, cols])\n","dfm.loc[dfm['DATETIME'] >= trainTestSplit, cols] = scaler.transform(dfm.loc[dfm['DATETIME'] >= trainTestSplit, cols])\n","\n","# Scale 3 CV DataFrames\n","# Both val and test sets will use the same scaled parameters learnt from training set\n","### 1618\n","scaler_1618 = MinMaxScaler()\n","df_1618.loc[df_1618['DATETIME'] <= train_end_1618, cols] = scaler_1618.fit_transform(df_1618.loc[df_1618['DATETIME'] <= train_end_1618, cols])\n","df_1618.loc[df_1618['DATETIME'] > train_end_1618, cols] = scaler_1618.transform(df_1618.loc[df_1618['DATETIME'] > train_end_1618, cols])\n","\n","### 1619\n","scaler_1619 = MinMaxScaler()\n","df_1619.loc[df_1619['DATETIME'] <= train_end_1619, cols] = scaler_1619.fit_transform(df_1619.loc[df_1619['DATETIME'] <= train_end_1619, cols])\n","df_1619.loc[df_1619['DATETIME'] > train_end_1619, cols] = scaler_1619.transform(df_1619.loc[df_1619['DATETIME'] > train_end_1619, cols])\n","\n","### 1620\n","scaler_1620 = MinMaxScaler()\n","df_1620.loc[df_1620['DATETIME'] <= train_end_1620, cols] = scaler_1620.fit_transform(df_1620.loc[df_1620['DATETIME'] <= train_end_1620, cols])\n","df_1620.loc[df_1620['DATETIME'] > train_end_1620, cols] = scaler_1620.transform(df_1620.loc[df_1620['DATETIME'] > train_end_1620, cols])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJcu4mCGph9I"},"source":["# Create function to scale predictions back to original scale - only used for final model once we find optimal hyperparameters\n","scale_min = scaler.data_min_[10:]\n","scale_max = scaler.data_max_[10:]\n","\n","def inv_transform(x, min_=scale_min, max_=scale_max):\n","    return x * (max_ - min_) + min_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"iAI1seyiON40","outputId":"b43ce1c2-98ff-4322-88e1-3dbcc293fb56"},"source":["# Check to make sure each DataFrame did scale \n","dfm\n","#df_1618\n","#df_1619\n","#df_1620"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>TEMPERATURE</th>\n","      <th>HOUR</th>\n","      <th>ISO_DAYOFWEEK</th>\n","      <th>MONTH</th>\n","      <th>QUARTER</th>\n","      <th>DAYOFYEAR</th>\n","      <th>DAYOFMONTH</th>\n","      <th>ISO_WEEKOFYEAR</th>\n","      <th>SEASON</th>\n","      <th>plus_30</th>\n","      <th>plus_60</th>\n","      <th>plus_90</th>\n","      <th>plus_120</th>\n","      <th>plus_150</th>\n","      <th>plus_180</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2016-01-01 00:00:00</td>\n","      <td>0.267854</td>\n","      <td>0.457801</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.239019</td>\n","      <td>0.222507</td>\n","      <td>0.193202</td>\n","      <td>0.183580</td>\n","      <td>0.173685</td>\n","      <td>0.164083</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016-01-01 00:30:00</td>\n","      <td>0.239019</td>\n","      <td>0.468031</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.222507</td>\n","      <td>0.193202</td>\n","      <td>0.183580</td>\n","      <td>0.173685</td>\n","      <td>0.164083</td>\n","      <td>0.157930</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2016-01-01 01:00:00</td>\n","      <td>0.222507</td>\n","      <td>0.455243</td>\n","      <td>0.043478</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.193202</td>\n","      <td>0.183580</td>\n","      <td>0.173685</td>\n","      <td>0.164083</td>\n","      <td>0.157930</td>\n","      <td>0.150700</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-01-01 01:30:00</td>\n","      <td>0.193202</td>\n","      <td>0.434783</td>\n","      <td>0.043478</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.183580</td>\n","      <td>0.173685</td>\n","      <td>0.164083</td>\n","      <td>0.157930</td>\n","      <td>0.150700</td>\n","      <td>0.152100</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2016-01-01 02:00:00</td>\n","      <td>0.183580</td>\n","      <td>0.429668</td>\n","      <td>0.086957</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.173685</td>\n","      <td>0.164083</td>\n","      <td>0.157930</td>\n","      <td>0.150700</td>\n","      <td>0.152100</td>\n","      <td>0.151235</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>90758</th>\n","      <td>2021-03-17 19:00:00</td>\n","      <td>0.569813</td>\n","      <td>0.475703</td>\n","      <td>0.826087</td>\n","      <td>0.333333</td>\n","      <td>0.181818</td>\n","      <td>0.0</td>\n","      <td>0.205479</td>\n","      <td>0.533333</td>\n","      <td>0.192308</td>\n","      <td>0.333333</td>\n","      <td>0.542405</td>\n","      <td>0.518941</td>\n","      <td>0.501041</td>\n","      <td>0.468034</td>\n","      <td>0.426128</td>\n","      <td>0.390433</td>\n","    </tr>\n","    <tr>\n","      <th>90759</th>\n","      <td>2021-03-17 19:30:00</td>\n","      <td>0.542405</td>\n","      <td>0.468031</td>\n","      <td>0.826087</td>\n","      <td>0.333333</td>\n","      <td>0.181818</td>\n","      <td>0.0</td>\n","      <td>0.205479</td>\n","      <td>0.533333</td>\n","      <td>0.192308</td>\n","      <td>0.333333</td>\n","      <td>0.518941</td>\n","      <td>0.501041</td>\n","      <td>0.468034</td>\n","      <td>0.426128</td>\n","      <td>0.390433</td>\n","      <td>0.359650</td>\n","    </tr>\n","    <tr>\n","      <th>90760</th>\n","      <td>2021-03-17 20:00:00</td>\n","      <td>0.518941</td>\n","      <td>0.468031</td>\n","      <td>0.869565</td>\n","      <td>0.333333</td>\n","      <td>0.181818</td>\n","      <td>0.0</td>\n","      <td>0.205479</td>\n","      <td>0.533333</td>\n","      <td>0.192308</td>\n","      <td>0.333333</td>\n","      <td>0.501041</td>\n","      <td>0.468034</td>\n","      <td>0.426128</td>\n","      <td>0.390433</td>\n","      <td>0.359650</td>\n","      <td>0.338932</td>\n","    </tr>\n","    <tr>\n","      <th>90761</th>\n","      <td>2021-03-17 20:30:00</td>\n","      <td>0.501041</td>\n","      <td>0.468031</td>\n","      <td>0.869565</td>\n","      <td>0.333333</td>\n","      <td>0.181818</td>\n","      <td>0.0</td>\n","      <td>0.205479</td>\n","      <td>0.533333</td>\n","      <td>0.192308</td>\n","      <td>0.333333</td>\n","      <td>0.468034</td>\n","      <td>0.426128</td>\n","      <td>0.390433</td>\n","      <td>0.359650</td>\n","      <td>0.338932</td>\n","      <td>0.296537</td>\n","    </tr>\n","    <tr>\n","      <th>90762</th>\n","      <td>2021-03-17 21:00:00</td>\n","      <td>0.468034</td>\n","      <td>0.468031</td>\n","      <td>0.913043</td>\n","      <td>0.333333</td>\n","      <td>0.181818</td>\n","      <td>0.0</td>\n","      <td>0.205479</td>\n","      <td>0.533333</td>\n","      <td>0.192308</td>\n","      <td>0.333333</td>\n","      <td>0.426128</td>\n","      <td>0.390433</td>\n","      <td>0.359650</td>\n","      <td>0.338932</td>\n","      <td>0.296537</td>\n","      <td>0.268915</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>90763 rows × 17 columns</p>\n","</div>"],"text/plain":["                 DATETIME  TOTALDEMAND  ...  plus_150  plus_180\n","0     2016-01-01 00:00:00     0.267854  ...  0.173685  0.164083\n","1     2016-01-01 00:30:00     0.239019  ...  0.164083  0.157930\n","2     2016-01-01 01:00:00     0.222507  ...  0.157930  0.150700\n","3     2016-01-01 01:30:00     0.193202  ...  0.150700  0.152100\n","4     2016-01-01 02:00:00     0.183580  ...  0.152100  0.151235\n","...                   ...          ...  ...       ...       ...\n","90758 2021-03-17 19:00:00     0.569813  ...  0.426128  0.390433\n","90759 2021-03-17 19:30:00     0.542405  ...  0.390433  0.359650\n","90760 2021-03-17 20:00:00     0.518941  ...  0.359650  0.338932\n","90761 2021-03-17 20:30:00     0.501041  ...  0.338932  0.296537\n","90762 2021-03-17 21:00:00     0.468034  ...  0.296537  0.268915\n","\n","[90763 rows x 17 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"noTjQo5XpsLn"},"source":["# Specifies the number of timesteps (i.e. half-hour intervals) to batch data by\n","n_steps_in = 48\n","\n","# Split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps_in, trainTestSplit, train_end=None, val_end=None, test_end=None, trainValTest=False):\n","    \n","    if trainValTest is False:\n","\n","        X_train, y_train = list(), list()\n","        X_test, y_test = list(), list()\n","        for i in range(len(sequences)):\n","\n","            # find the end of this pattern\n","            end_ix = i + n_steps_in\n","\n","            # check if we are beyond the dataset\n","            if end_ix > len(sequences):\n","                break\n","\n","            # Gets the DATETIME of the prediction in which the model will make \n","            pred_datetime = str(sequences['DATETIME'][end_ix-1])\n","            \n","            # Defines training data\n","            if pred_datetime < trainTestSplit:\n","                # gather input and output parts of the pattern\n","                seq_x, seq_y = sequences.iloc[i:end_ix, 1:11], sequences.iloc[end_ix-1, 11:]\n","                X_train.append(seq_x)\n","                y_train.append(seq_y)\n","\n","            # Define testing data\n","            elif pred_datetime >= trainTestSplit:\n","                # gather input and output parts of the pattern\n","                seq_x, seq_y = sequences.iloc[i:end_ix, 1:11], sequences.iloc[end_ix-1, 11:]\n","                X_test.append(seq_x)\n","                y_test.append(seq_y)\n","\n","        return array(X_train), array(y_train), array(X_test), array(y_test)\n","\n","    else:\n","\n","        X_train, y_train = list(), list()\n","        X_val, y_val = list(), list()\n","        X_test, y_test = list(), list()\n","        for i in range(len(sequences)):\n","\n","            # find the end of this pattern\n","            end_ix = i + n_steps_in\n","\n","            # check if we are beyond the dataset\n","            if end_ix > len(sequences):\n","                break\n","\n","            # Gets the DATETIME of the prediction in which the model will make \n","            pred_datetime = str(sequences['DATETIME'][end_ix-1])\n","            \n","            # Defines training data\n","            if pred_datetime <= train_end:\n","                # gather input and output parts of the pattern\n","                seq_x, seq_y = sequences.iloc[i:end_ix, 1:11], sequences.iloc[end_ix-1, 11:]\n","                X_train.append(seq_x)\n","                y_train.append(seq_y)\n","\n","            # Define testing data\n","            elif (train_end < pred_datetime) and (pred_datetime <= val_end):\n","                # gather input and output parts of the pattern\n","                seq_x, seq_y = sequences.iloc[i:end_ix, 1:11], sequences.iloc[end_ix-1, 11:]\n","                X_val.append(seq_x)\n","                y_val.append(seq_y)\n","\n","            elif (val_end < pred_datetime) and (pred_datetime <= test_end):\n","                # gather input and output parts of the pattern\n","                seq_x, seq_y = sequences.iloc[i:end_ix, 1:11], sequences.iloc[end_ix-1, 11:]\n","                X_test.append(seq_x)\n","                y_test.append(seq_y)\n","    \n","        return array(X_train), array(y_train), array(X_val), array(y_val), array(X_test), array(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWbC9mcyf-1Y"},"source":["# This cell will take a few minutes to run\n","\n","# Create main train/test dataframes\n","X_train, y_train, X_test, y_test = split_sequences(dfm, n_steps_in, trainTestSplit) \n","\n","# Create train/validation dataframes\n","X_train_1618, y_train_1618, X_val_1618, y_val_1618, X_test_1618, y_test_1618 = split_sequences(df_1618, n_steps_in, trainTestSplit=None,\n","                                                                                               train_end=train_end_1618, val_end=val_end_1618, test_end=test_end_1618, trainValTest=True)\n","X_train_1619, y_train_1619, X_val_1619, y_val_1619, X_test_1619, y_test_1619 = split_sequences(df_1619, n_steps_in, trainTestSplit=None,\n","                                                                                               train_end=train_end_1619, val_end=val_end_1619, test_end=test_end_1619, trainValTest=True) \n","X_train_1620, y_train_1620, X_val_1620, y_val_1620, X_test_1620, y_test_1620 = split_sequences(df_1620, n_steps_in, trainTestSplit=None,\n","                                                                                               train_end=train_end_1620, val_end=val_end_1620, test_end=test_end_1620, trainValTest=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-R7H7KUWru9Z","outputId":"da76d8cc-172d-489f-c066-53113e6472ce"},"source":["print('16-21')\n","print(f'Train:\\nX has shape: {X_train.shape}\\ny has shape: {y_train.shape}\\n')\n","print(f'Test:\\nX has shape: {X_test.shape}\\ny has shape: {y_test.shape}')\n","\n","print('\\n\\n16-18')\n","print(f'Train:\\nX has shape: {X_train_1618.shape}\\ny has shape: {y_train_1618.shape}\\n')\n","print(f'Validation:\\nX has shape: {X_val_1618.shape}\\ny has shape: {y_val_1618.shape}')\n","print(f'Test:\\nX has shape: {X_test_1618.shape}\\ny has shape: {y_test_1618.shape}')\n","\n","print('\\n\\n16-19')\n","print(f'Train:\\nX has shape: {X_train_1619.shape}\\ny has shape: {y_train_1619.shape}\\n')\n","print(f'Validation:\\nX has shape: {X_val_1619.shape}\\ny has shape: {y_val_1619.shape}')\n","print(f'Test:\\nX has shape: {X_test_1619.shape}\\ny has shape: {y_test_1619.shape}')\n","\n","print('\\n\\n16-20')\n","print(f'Train:\\nX has shape: {X_train_1620.shape}\\ny has shape: {y_train_1620.shape}\\n')\n","print(f'Validation:\\nX has shape: {X_val_1620.shape}\\ny has shape: {y_val_1620.shape}')\n","print(f'Test:\\nX has shape: {X_test_1620.shape}\\ny has shape: {y_test_1620.shape}')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["16-21\n","Train:\n","X has shape: (69553, 48, 10)\n","y has shape: (69553, 6)\n","\n","Test:\n","X has shape: (21163, 48, 10)\n","y has shape: (21163, 6)\n","\n","\n","16-18\n","Train:\n","X has shape: (16993, 48, 10)\n","y has shape: (16993, 6)\n","\n","Validation:\n","X has shape: (17520, 48, 10)\n","y has shape: (17520, 6)\n","Test:\n","X has shape: (17520, 48, 10)\n","y has shape: (17520, 6)\n","\n","\n","16-19\n","Train:\n","X has shape: (34513, 48, 10)\n","y has shape: (34513, 6)\n","\n","Validation:\n","X has shape: (17520, 48, 10)\n","y has shape: (17520, 6)\n","Test:\n","X has shape: (17520, 48, 10)\n","y has shape: (17520, 6)\n","\n","\n","16-20\n","Train:\n","X has shape: (52033, 48, 10)\n","y has shape: (52033, 6)\n","\n","Validation:\n","X has shape: (17520, 48, 10)\n","y has shape: (17520, 6)\n","Test:\n","X has shape: (21163, 48, 10)\n","y has shape: (21163, 6)\n"]}]},{"cell_type":"code","metadata":{"id":"EtQd_LnVr4M0"},"source":["# Convert each of the numpy arrays to tensors\n","X_train_tf = tf.convert_to_tensor(X_train, np.float32)\n","y_train_tf = tf.convert_to_tensor(y_train, np.float32)\n","X_test_tf = tf.convert_to_tensor(X_test, np.float32) \n","y_test_tf = tf.convert_to_tensor(y_test, np.float32) \n","\n","X_train_tf_1618 = tf.convert_to_tensor(X_train_1618, np.float32)\n","y_train_tf_1618 = tf.convert_to_tensor(y_train_1618, np.float32)\n","X_val_tf_1618 = tf.convert_to_tensor(X_val_1618, np.float32) \n","y_val_tf_1618 = tf.convert_to_tensor(y_val_1618, np.float32) \n","X_test_tf_1618 = tf.convert_to_tensor(X_test_1618, np.float32) \n","y_test_tf_1618 = tf.convert_to_tensor(y_test_1618, np.float32) \n","\n","X_train_tf_1619 = tf.convert_to_tensor(X_train_1619, np.float32)\n","y_train_tf_1619 = tf.convert_to_tensor(y_train_1619, np.float32)\n","X_val_tf_1619 = tf.convert_to_tensor(X_val_1619, np.float32) \n","y_val_tf_1619 = tf.convert_to_tensor(y_val_1619, np.float32) \n","X_test_tf_1619 = tf.convert_to_tensor(X_test_1619, np.float32) \n","y_test_tf_1619 = tf.convert_to_tensor(y_test_1619, np.float32)\n","\n","X_train_tf_1620 = tf.convert_to_tensor(X_train_1620, np.float32)\n","y_train_tf_1620 = tf.convert_to_tensor(y_train_1620, np.float32)\n","X_val_tf_1620 = tf.convert_to_tensor(X_val_1620, np.float32) \n","y_val_tf_1620 = tf.convert_to_tensor(y_val_1620, np.float32) \n","X_test_tf_1620 = tf.convert_to_tensor(X_test_1620, np.float32) \n","y_test_tf_1620 = tf.convert_to_tensor(y_test_1620, np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9H1zdTJWr4QC"},"source":["# Create train and test Datasets batched with 144 observations (48 half-hours/day * 3 days)\n","batch_size = 48*3  \n","\n","dataset_train_tf = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf)).batch(batch_size)\n","dataset_test_tf = tf.data.Dataset.from_tensor_slices((X_test_tf, y_test_tf)).batch(batch_size)\n","\n","dataset_train_tf_1618 = tf.data.Dataset.from_tensor_slices((X_train_tf_1618, y_train_tf_1618)).batch(batch_size)\n","dataset_val_tf_1618 = tf.data.Dataset.from_tensor_slices((X_val_tf_1618, y_val_tf_1618)).batch(batch_size)\n","dataset_test_tf_1618 = tf.data.Dataset.from_tensor_slices((X_test_tf_1618, y_test_tf_1618)).batch(batch_size)\n","\n","dataset_train_tf_1619 = tf.data.Dataset.from_tensor_slices((X_train_tf_1619, y_train_tf_1619)).batch(batch_size)\n","dataset_val_tf_1619 = tf.data.Dataset.from_tensor_slices((X_val_tf_1619, y_val_tf_1619)).batch(batch_size)\n","dataset_test_tf_1619 = tf.data.Dataset.from_tensor_slices((X_test_tf_1619, y_test_tf_1619)).batch(batch_size)\n","\n","dataset_train_tf_1620 = tf.data.Dataset.from_tensor_slices((X_train_tf_1620, y_train_tf_1620)).batch(batch_size)\n","dataset_val_tf_1620 = tf.data.Dataset.from_tensor_slices((X_val_tf_1620, y_val_tf_1620)).batch(batch_size)\n","dataset_test_tf_1620 = tf.data.Dataset.from_tensor_slices((X_test_tf_1620, y_test_tf_1620)).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLrZk7r6f-1Z"},"source":["## Defining the 3-Fold Nested CV Loop"]},{"cell_type":"code","metadata":{"id":"CgDc-h6af-1b"},"source":["# Define callback for early stopping\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                            patience=3,\n","                                            restore_best_weights=True,\n","                                            min_delta=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HbmafUf-1c"},"source":["# Define model inside function\n","def run_model_trainVal(df_train, df_val, df_test, epochs, n_features, activation, hid_neurons, optimizer, n_steps_in=n_steps_in, n_steps_out=6):\n","    # Define dict to put in train, val, and test accuracies/losses\n","    trainValTestLoss = dict()\n","    \n","    # Define model\n","    model = Sequential()\n","    model.add(LSTM(hid_neurons, activation=activation, return_sequences=True, input_shape=(n_steps_in, n_features)))\n","    model.add(LSTM(hid_neurons, activation=activation))\n","    model.add(Dense(n_steps_out))\n","\n","    # Compile model\n","    # model.compile(optimizer=optimizer, loss=root_mean_squared_error)\n","    model.compile(optimizer=optimizer, loss='mean_absolute_error')\n","\n","    # Fit model\n","    history = model.fit(df_train, validation_data=df_val, epochs=epochs, callbacks=[callback])\n","\n","    # Evaluate model on testing data\n","    test_loss = model.evaluate(df_test)\n","\n","    # Update trainValTestAcc dict\n","    trainValTestLoss.update(history.history)\n","    trainValTestLoss['test_loss'] = test_loss\n","\n","    return trainValTestLoss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"53n0mohqf-1e"},"source":["#################################################################################\n","########### You will need to manually change the parameters each time ###########\n","#################################################################################\n","epochs = 50\n","n_features = X_train.shape[2]\n","activation = 'tanh' # GPU only works for tanh activation\n","hid_neurons = 50\n","optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","# Don't forget \"batch_size\" three code cells above!\n","\n","datasets = {\n","    \n","    'model_1618': {\n","        'df_train': dataset_train_tf_1618,\n","        'df_val': dataset_val_tf_1618,\n","        'df_test': dataset_test_tf_1618,\n","    },\n","    \n","    'model_1619': {\n","        'df_train': dataset_train_tf_1619,\n","        'df_val': dataset_val_tf_1619,\n","        'df_test': dataset_test_tf_1619,\n","    },\n","    \n","    'model_1620': {\n","        'df_train': dataset_train_tf_1620,\n","        'df_val': dataset_val_tf_1620,\n","        'df_test': dataset_test_tf_1620,\n","    }\n","    \n","}\n","\n","model_losses = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"s2p8aczQf-1f","outputId":"8ba33a6e-269c-4d06-ba42-e945440bfd92"},"source":["for model, dataset in datasets.items():\n","    print(f'Running {model}\\n\\n')\n","    history = run_model_trainVal(df_train = dataset['df_train'], \n","                                 df_val = dataset['df_val'], \n","                                 df_test = dataset['df_test'],\n","                                 epochs = epochs, \n","                                 n_features = n_features, \n","                                 activation = activation,\n","                                 hid_neurons = hid_neurons,\n","                                 optimizer = optimizer)\n","    model_losses[model] = history\n","    print('\\n\\n')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Running model_1618\n","\n","\n","119/119 [==============================] - 18s 123ms/step - loss: 0.1385 - val_loss: 0.1887\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-5555e3d37720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                  \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                  \u001b[0mhid_neurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_neurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                  optimizer = optimizer)\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-cd22d6a44490>\u001b[0m in \u001b[0;36mrun_model_trainVal\u001b[0;34m(df_train, df_val, df_test, epochs, n_features, activation, hid_neurons, optimizer, n_steps_in, n_steps_out)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Evaluate model on testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"FwlJhzKPf-1g","outputId":"9beae237-6881-4569-f137-6445e5ed038e"},"source":["count = 0\n","train_loss, val_loss, test_loss = 0, 0, 0\n","\n","for k,v in model_losses.items():\n","    count += 1\n","\n","    # Calculate val_loss, and index corresponding to minimum val_loss to get corresponding train_loss \n","    val_index = np.argmin(v['val_loss'])\n","    val_loss += np.min(v['val_loss'])\n","\n","    train_loss += v['loss'][val_index]\n","    test_loss += v['test_loss']\n","  \n","print(f'Average Training Loss: {train_loss / count}')\n","print(f'Average Validation Loss: {val_loss / count}')\n","print(f'Average Testing Loss: {test_loss / count}')"],"execution_count":null,"outputs":[{"ename":"ZeroDivisionError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-cd9ff20af753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Average Training Loss: {train_loss / count}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Average Validation Loss: {val_loss / count}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Average Testing Loss: {test_loss / count}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"markdown","metadata":{"id":"pOeZQ-GWNs7O"},"source":["# Training and Testing - Using Optimal Hyperparameters from above"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z15fqcLTf3Ti","outputId":"46090e94-40bd-45a4-b65b-4ff2aaa1d515"},"source":["for i, j in dataset_train_tf:\n","  print(i.shape, j.shape)\n","  break"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(144, 48, 10) (144, 6)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-ADnvhHf-1i","outputId":"4c79d8e1-081a-4afb-9155-8d975cb5c005"},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', \n","                                            patience=3,\n","                                            restore_best_weights=True,\n","                                            min_delta=0.005)\n","epochs = 1 # 10, 15, 25\n","\n","### Define training/testing model\n","# Define dict to put in train, val, and test accuracies/losses\n","trainValTestLoss = dict()\n","\n","# Define model\n","model = Sequential()\n","model.add(LSTM(50, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))) # 48, 10\n","model.add(LSTM(50, activation='tanh'))\n","model.add(Dense(6))\n","\n","# Compile model\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n","              loss='mean_absolute_error')\n","\n","# Fit model\n","history = model.fit(dataset_train_tf, epochs=epochs)\n","\n","# Evaluate model on testing data\n","test_loss = model.evaluate(dataset_test_tf)\n","\n","# Update trainValTestAcc dict\n","trainValTestLoss.update(history.history)\n","trainValTestLoss['test_loss'] = test_loss"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["484/484 [==============================] - 45s 86ms/step - loss: 0.0676\n","147/147 [==============================] - 6s 32ms/step - loss: 0.0800\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MAZascY8s4e","outputId":"c8e44d85-1ddd-4324-d45d-a701db25051c"},"source":["trainValTestLoss"],"execution_count":null,"outputs":[{"data":{"text/plain":["{'loss': [0.0676279217004776], 'test_loss': 0.08002595603466034}"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"Bl9wXYy34gJw"},"source":["# Create function to scale predictions back to original scale\n","scale_min = scaler.data_min_[10:]\n","scale_max = scaler.data_max_[10:]\n","\n","def inv_transform(df, min_=scale_min, max_=scale_max):\n","  for i in range(len(min_)):\n","    df[:, i] = df[:, i] * (max_[i] - min_[i]) + min_[i]\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vs6_kFY2DVnM"},"source":["# Calculates the residuals\n","def residuals(df, max=6*30+30):\n","  for i in range(30, max, 30):\n","    df[f'residuals_{i}'] = df[f'plus_{i}'] - df[f'pred_{i}']\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"2okbzOUUSGaT","outputId":"089d21ae-e87b-4ea5-f726-dfcc484ae794"},"source":["# Observing DataFrame, which we defined earlier, to append predictions onto\n","dfm3"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>TEMPERATURE</th>\n","      <th>HOUR</th>\n","      <th>ISO_DAYOFWEEK</th>\n","      <th>MONTH</th>\n","      <th>QUARTER</th>\n","      <th>DAYOFYEAR</th>\n","      <th>DAYOFMONTH</th>\n","      <th>ISO_WEEKOFYEAR</th>\n","      <th>SEASON</th>\n","      <th>plus_30</th>\n","      <th>plus_60</th>\n","      <th>plus_90</th>\n","      <th>plus_120</th>\n","      <th>plus_150</th>\n","      <th>plus_180</th>\n","    </tr>\n","    <tr>\n","      <th>DATETIME</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2016-01-01 00:00:00</th>\n","      <td>2016-01-01 00:00:00</td>\n","      <td>5730.86</td>\n","      <td>19.4</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5563.19</td>\n","      <td>5467.18</td>\n","      <td>5296.78</td>\n","      <td>5240.83</td>\n","      <td>5183.29</td>\n","      <td>5127.46</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-01 00:30:00</th>\n","      <td>2016-01-01 00:30:00</td>\n","      <td>5563.19</td>\n","      <td>19.8</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5467.18</td>\n","      <td>5296.78</td>\n","      <td>5240.83</td>\n","      <td>5183.29</td>\n","      <td>5127.46</td>\n","      <td>5091.68</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-01 01:00:00</th>\n","      <td>2016-01-01 01:00:00</td>\n","      <td>5467.18</td>\n","      <td>19.3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5296.78</td>\n","      <td>5240.83</td>\n","      <td>5183.29</td>\n","      <td>5127.46</td>\n","      <td>5091.68</td>\n","      <td>5049.64</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-01 01:30:00</th>\n","      <td>2016-01-01 01:30:00</td>\n","      <td>5296.78</td>\n","      <td>18.5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5240.83</td>\n","      <td>5183.29</td>\n","      <td>5127.46</td>\n","      <td>5091.68</td>\n","      <td>5049.64</td>\n","      <td>5057.78</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-01 02:00:00</th>\n","      <td>2016-01-01 02:00:00</td>\n","      <td>5240.83</td>\n","      <td>18.3</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>5183.29</td>\n","      <td>5127.46</td>\n","      <td>5091.68</td>\n","      <td>5049.64</td>\n","      <td>5057.78</td>\n","      <td>5052.75</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2021-03-17 19:00:00</th>\n","      <td>2021-03-17 19:00:00</td>\n","      <td>7486.67</td>\n","      <td>20.1</td>\n","      <td>19</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7327.30</td>\n","      <td>7190.86</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","    </tr>\n","    <tr>\n","      <th>2021-03-17 19:30:00</th>\n","      <td>2021-03-17 19:30:00</td>\n","      <td>7327.30</td>\n","      <td>19.8</td>\n","      <td>19</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7190.86</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","    </tr>\n","    <tr>\n","      <th>2021-03-17 20:00:00</th>\n","      <td>2021-03-17 20:00:00</td>\n","      <td>7190.86</td>\n","      <td>19.8</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>7086.78</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","    </tr>\n","    <tr>\n","      <th>2021-03-17 20:30:00</th>\n","      <td>2021-03-17 20:30:00</td>\n","      <td>7086.78</td>\n","      <td>19.8</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>6894.85</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","      <td>5897.64</td>\n","    </tr>\n","    <tr>\n","      <th>2021-03-17 21:00:00</th>\n","      <td>2021-03-17 21:00:00</td>\n","      <td>6894.85</td>\n","      <td>19.8</td>\n","      <td>21</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>17</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>6651.18</td>\n","      <td>6443.62</td>\n","      <td>6264.63</td>\n","      <td>6144.16</td>\n","      <td>5897.64</td>\n","      <td>5737.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>90763 rows × 17 columns</p>\n","</div>"],"text/plain":["                               DATETIME  TOTALDEMAND  ...  plus_150  plus_180\n","DATETIME                                              ...                    \n","2016-01-01 00:00:00 2016-01-01 00:00:00      5730.86  ...   5183.29   5127.46\n","2016-01-01 00:30:00 2016-01-01 00:30:00      5563.19  ...   5127.46   5091.68\n","2016-01-01 01:00:00 2016-01-01 01:00:00      5467.18  ...   5091.68   5049.64\n","2016-01-01 01:30:00 2016-01-01 01:30:00      5296.78  ...   5049.64   5057.78\n","2016-01-01 02:00:00 2016-01-01 02:00:00      5240.83  ...   5057.78   5052.75\n","...                                 ...          ...  ...       ...       ...\n","2021-03-17 19:00:00 2021-03-17 19:00:00      7486.67  ...   6651.18   6443.62\n","2021-03-17 19:30:00 2021-03-17 19:30:00      7327.30  ...   6443.62   6264.63\n","2021-03-17 20:00:00 2021-03-17 20:00:00      7190.86  ...   6264.63   6144.16\n","2021-03-17 20:30:00 2021-03-17 20:30:00      7086.78  ...   6144.16   5897.64\n","2021-03-17 21:00:00 2021-03-17 21:00:00      6894.85  ...   5897.64   5737.03\n","\n","[90763 rows x 17 columns]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"as2bnThP8s8h"},"source":["# Calculates train and test predictions\n","preds_train = model.predict(X_train_tf) \n","preds_test = model.predict(X_test_tf) \n","\n","# Get cols - used to create new DataFrame with predictions appended\n","colsPred = ['DATETIME', 'TOTALDEMAND'] + [f'plus_{x}' for x in range(30, 7*30, 30)]\n","\n","# Create new DataFrame with just DATETIMES relating to train predictions, then scale back predictions to original scale, then append Residual columns\n","train_preds = dfm3[colsPred][n_steps_in-1:len(preds_train)+n_steps_in-1].copy()\n","train_preds[[f'pred_{x}' for x in range(30, 7*30, 30)]] = inv_transform(preds_train)\n","train_preds = residuals(train_preds)\n","\n","# Create new DataFrame with just DATETIMES relating to test predictions, then scale back predictions to original scale, then append Residual columns\n","test_preds = dfm3[colsPred][len(preds_train)+n_steps_in-1:].copy()\n","test_preds[[f'pred_{x}' for x in range(30, 7*30, 30)]] = inv_transform(preds_test)\n","test_preds = residuals(test_preds)\n","\n","# Adding Residuals / plus (for MAPE)\n","for i in range(30, 7*30, 30):\n","  test_preds[f'resid_over_actual_{i}'] = test_preds[f'residuals_{i}'] / test_preds[f'plus_{i}']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"5xDbYV2JYd5Z","outputId":"77733299-7635-4f23-b890-b40fe8fa3de7"},"source":["# Observe DataFrame related to training predictions\n","train_preds.head(2)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>plus_30</th>\n","      <th>plus_60</th>\n","      <th>plus_90</th>\n","      <th>plus_120</th>\n","      <th>plus_150</th>\n","      <th>plus_180</th>\n","      <th>pred_30</th>\n","      <th>pred_60</th>\n","      <th>pred_90</th>\n","      <th>pred_120</th>\n","      <th>pred_150</th>\n","      <th>pred_180</th>\n","      <th>residuals_30</th>\n","      <th>residuals_60</th>\n","      <th>residuals_90</th>\n","      <th>residuals_120</th>\n","      <th>residuals_150</th>\n","      <th>residuals_180</th>\n","    </tr>\n","    <tr>\n","      <th>DATETIME</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2016-01-01 23:30:00</th>\n","      <td>2016-01-01 23:30:00</td>\n","      <td>5809.78</td>\n","      <td>5625.79</td>\n","      <td>5418.92</td>\n","      <td>5287.29</td>\n","      <td>5123.09</td>\n","      <td>5019.33</td>\n","      <td>5002.42</td>\n","      <td>6431.719727</td>\n","      <td>6057.678711</td>\n","      <td>6053.183594</td>\n","      <td>6222.263672</td>\n","      <td>5914.086914</td>\n","      <td>5949.947754</td>\n","      <td>-805.929727</td>\n","      <td>-638.758711</td>\n","      <td>-765.893594</td>\n","      <td>-1099.173672</td>\n","      <td>-894.756914</td>\n","      <td>-947.527754</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-02 00:00:00</th>\n","      <td>2016-01-02 00:00:00</td>\n","      <td>5625.79</td>\n","      <td>5418.92</td>\n","      <td>5287.29</td>\n","      <td>5123.09</td>\n","      <td>5019.33</td>\n","      <td>5002.42</td>\n","      <td>4961.19</td>\n","      <td>6278.652832</td>\n","      <td>5922.485840</td>\n","      <td>5902.059082</td>\n","      <td>6098.519531</td>\n","      <td>5820.501953</td>\n","      <td>5894.673340</td>\n","      <td>-859.732832</td>\n","      <td>-635.195840</td>\n","      <td>-778.969082</td>\n","      <td>-1079.189531</td>\n","      <td>-818.081953</td>\n","      <td>-933.483340</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               DATETIME  ...  residuals_180\n","DATETIME                                 ...               \n","2016-01-01 23:30:00 2016-01-01 23:30:00  ...    -947.527754\n","2016-01-02 00:00:00 2016-01-02 00:00:00  ...    -933.483340\n","\n","[2 rows x 20 columns]"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"frHETRu4f-1i","outputId":"e624943d-4cf2-4b3a-a518-262ee5918b67"},"source":["# Observe DataFrame related to testing predictions\n","test_preds.head(2)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DATETIME</th>\n","      <th>TOTALDEMAND</th>\n","      <th>plus_30</th>\n","      <th>plus_60</th>\n","      <th>plus_90</th>\n","      <th>plus_120</th>\n","      <th>plus_150</th>\n","      <th>plus_180</th>\n","      <th>pred_30</th>\n","      <th>pred_60</th>\n","      <th>pred_90</th>\n","      <th>pred_120</th>\n","      <th>pred_150</th>\n","      <th>pred_180</th>\n","      <th>residuals_30</th>\n","      <th>residuals_60</th>\n","      <th>residuals_90</th>\n","      <th>residuals_120</th>\n","      <th>residuals_150</th>\n","      <th>residuals_180</th>\n","      <th>resid_over_actual_30</th>\n","      <th>resid_over_actual_60</th>\n","      <th>resid_over_actual_90</th>\n","      <th>resid_over_actual_120</th>\n","      <th>resid_over_actual_150</th>\n","      <th>resid_over_actual_180</th>\n","    </tr>\n","    <tr>\n","      <th>DATETIME</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2020-01-01 00:00:00</th>\n","      <td>2020-01-01 00:00:00</td>\n","      <td>6218.39</td>\n","      <td>6029.36</td>\n","      <td>5954.32</td>\n","      <td>5888.68</td>\n","      <td>5820.63</td>\n","      <td>5750.66</td>\n","      <td>5705.83</td>\n","      <td>5939.029785</td>\n","      <td>5903.852539</td>\n","      <td>5669.841309</td>\n","      <td>5778.642578</td>\n","      <td>5593.921875</td>\n","      <td>5465.144531</td>\n","      <td>90.330215</td>\n","      <td>50.467461</td>\n","      <td>218.838691</td>\n","      <td>41.987422</td>\n","      <td>156.738125</td>\n","      <td>240.685469</td>\n","      <td>0.014982</td>\n","      <td>0.008476</td>\n","      <td>0.037163</td>\n","      <td>0.007214</td>\n","      <td>0.027256</td>\n","      <td>0.042182</td>\n","    </tr>\n","    <tr>\n","      <th>2020-01-01 00:30:00</th>\n","      <td>2020-01-01 00:30:00</td>\n","      <td>6029.36</td>\n","      <td>5954.32</td>\n","      <td>5888.68</td>\n","      <td>5820.63</td>\n","      <td>5750.66</td>\n","      <td>5705.83</td>\n","      <td>5662.17</td>\n","      <td>5894.525391</td>\n","      <td>5839.777344</td>\n","      <td>5672.261230</td>\n","      <td>5798.119141</td>\n","      <td>5631.458984</td>\n","      <td>5547.957031</td>\n","      <td>59.794609</td>\n","      <td>48.902656</td>\n","      <td>148.368770</td>\n","      <td>-47.459141</td>\n","      <td>74.371016</td>\n","      <td>114.212969</td>\n","      <td>0.010042</td>\n","      <td>0.008305</td>\n","      <td>0.025490</td>\n","      <td>-0.008253</td>\n","      <td>0.013034</td>\n","      <td>0.020171</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               DATETIME  ...  resid_over_actual_180\n","DATETIME                                 ...                       \n","2020-01-01 00:00:00 2020-01-01 00:00:00  ...               0.042182\n","2020-01-01 00:30:00 2020-01-01 00:30:00  ...               0.020171\n","\n","[2 rows x 26 columns]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"PzVdAeMY8tIo"},"source":["# Create functions to calculate RMSE, MSE, MAE, MAPE, R-Squared\n","def RMSE(dfPreds, cols=[f'residuals_{x}' for x in range(30, 30*6+30, 30)]):\n","  RMSE = dfPreds[cols].apply(lambda x: x**2, axis=1).agg('mean').apply(lambda x: np.sqrt(x)).mean()\n","  return RMSE\n","\n","def MSE(dfPreds, cols=[f'residuals_{x}' for x in range(30, 30*6+30, 30)]):\n","  MSE = dfPreds[cols].apply(lambda x: x**2, axis=1).agg('mean').mean()\n","  return MSE\n","\n","def MAE(dfPreds, cols=[f'residuals_{x}' for x in range(30, 30*6+30, 30)]):\n","  MAE = dfPreds[cols].apply(lambda x: np.abs(x), axis=1).agg('mean').mean()\n","  return MAE\n","\n","def MAPE(dfPreds, cols=[f'resid_over_actual_{x}' for x in range(30, 30*6+30, 30)]):\n","  MAPE = dfPreds[cols].apply(lambda x: np.abs(x), axis=1).agg('mean').mean()\n","  return MAPE\n","\n","def Rsq(df, \n","        cols=[f'residuals_{x}' for x in range(30, 30*6+30, 30)],\n","        cols2=[f'plus_{x}' for x in range(30, 30*6+30, 30)]):\n","  MSE = df[cols].apply(lambda x: x**2, axis=1).agg('mean').mean()\n","  VAR = df[cols2].apply(lambda x: np.var(x)).mean()\n","  R_Squared = 1 - (MSE / VAR)\n","  return R_Squared\n","\n","RMSE = RMSE(test_preds)\n","MSE = MSE(test_preds)\n","MAE = MAE(test_preds)\n","MAPE = MAPE(test_preds)\n","R_Squared = Rsq(test_preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CU17qWNi8tRJ","outputId":"5062124d-59a6-45a4-820f-ebbf9d1b78c6"},"source":["print(f'RMSE: {RMSE}\\n'\n","      f'MSE: {MSE}\\n'\n","      f'MAE: {MAE}\\n'\n","      f'MAPE: {MAPE}\\n'\n","      f'Accuracy: {1 - MAPE}\\n'\n","      f'R Squared: {R_Squared}')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE: 588.341835031303\n","MSE: 349173.53394970484\n","MAE: 465.3292480931747\n","MAPE: 0.07847595483697338\n","Accuracy: 0.9215240451630267\n","R Squared: 0.6170770000303334\n"]}]}]}